{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bd2826",
   "metadata": {
    "papermill": {
     "duration": 0.00459,
     "end_time": "2025-07-11T07:54:38.299879",
     "exception": false,
     "start_time": "2025-07-11T07:54:38.295289",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# LeNet-5 (1998) – NumPy Implementation from Scratch\n",
    "\n",
    "In this notebook I build LeNet-5, the CNN introduced by\n",
    "Yann LeCun for handwritten digit recognition.\n",
    "\n",
    "* **Dataset** : MNIST (60 k train / 10 k test)  \n",
    "* **Input size** : LeNet expects 32 × 32 greyscale images.  \n",
    "  MNIST is 28 × 28, so we pad 2 pixels on each side in the first\n",
    "  convolution layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9090e",
   "metadata": {
    "papermill": {
     "duration": 0.003396,
     "end_time": "2025-07-11T07:54:38.307140",
     "exception": false,
     "start_time": "2025-07-11T07:54:38.303744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7226dc",
   "metadata": {
    "papermill": {
     "duration": 0.003752,
     "end_time": "2025-07-11T07:54:38.314599",
     "exception": false,
     "start_time": "2025-07-11T07:54:38.310847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "| Layer      | Type                 | Parameters                             | Output Shape (input 28x28) |\n",
    "| ---------- | -------------------- | -------------------------------------- | -------------------------- |\n",
    "| **C1**     | Convolution          | 6 filters, 5×5 kernel, stride=1, pad=2 | (6, 28, 28)                |\n",
    "|            | Activation (Sigmoid) |                                        | (6, 28, 28)                |\n",
    "| **S2**     | Average Pooling      | 2×2 window, stride=2                   | (6, 14, 14)                |\n",
    "| **C3**     | Convolution          | 16 filters, 5×5 kernel, stride=1       | (16, 10, 10)               |\n",
    "|            | Activation (Sigmoid) |                                        | (16, 10, 10)               |\n",
    "| **S4**     | Average Pooling      | 2×2 window, stride=2                   | (16, 5, 5)                 |\n",
    "| **C5**     | Convolution          | 120 filters, 5×5 kernel, stride=1      | (120, 1, 1)                |\n",
    "|            | Activation (Sigmoid) |                                        | (120,)                     |\n",
    "| **F6**     | Fully Connected      | 120 → 84                               | (84,)                      |\n",
    "|            | Activation (Sigmoid) |                                        | (84,)                      |\n",
    "| **Output** | Fully Connected      | 84 → 10                                | (10,)                      |\n",
    "\n",
    "![Architecture](figures/Architecture.png)\n",
    "\n",
    "Image source: Zhang, Aston and Lipton, Zachary C. and Li, Mu and Smola, Alexander J. - https://github.com/d2l-ai/d2l-en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c6f09d",
   "metadata": {
    "papermill": {
     "duration": 0.00475,
     "end_time": "2025-07-11T07:54:38.323096",
     "exception": false,
     "start_time": "2025-07-11T07:54:38.318346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1  Load and preprocess MNIST\n",
    "Load the MNIST dataset from sklearn, normalize it to [0,1], and reshape for convolution layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e775321b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:54:38.332944Z",
     "iopub.status.busy": "2025-07-11T07:54:38.332198Z",
     "iopub.status.idle": "2025-07-11T07:55:08.669663Z",
     "shell.execute_reply": "2025-07-11T07:55:08.668546Z"
    },
    "papermill": {
     "duration": 30.344666,
     "end_time": "2025-07-11T07:55:08.671529",
     "exception": false,
     "start_time": "2025-07-11T07:54:38.326863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n",
    "\n",
    "X = mnist['data']       # Shape: (70000, 784)\n",
    "y = mnist['target']     # Shape: (70000,)\n",
    "\n",
    "X = X / 255.0           # Normalize pixel values to [0, 1]\n",
    "y = y.astype(np.int32)  # Convert labels to integers\n",
    "\n",
    "X = X.reshape(-1, 1, 28, 28) # Reshape for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f023e174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.681482Z",
     "iopub.status.busy": "2025-07-11T07:55:08.681079Z",
     "iopub.status.idle": "2025-07-11T07:55:08.686795Z",
     "shell.execute_reply": "2025-07-11T07:55:08.685832Z"
    },
    "papermill": {
     "duration": 0.012586,
     "end_time": "2025-07-11T07:55:08.688427",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.675841",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (10000, 1, 28, 28)\n",
      "X_test shape: (60000, 1, 28, 28)\n",
      "y_train shape: (10000,)\n",
      "y_test shape: (60000,)\n"
     ]
    }
   ],
   "source": [
    "# Split into train/test (60k train, 10k test)\n",
    "X_train, X_test = X[:10000], X[10000:]\n",
    "y_train, y_test = y[:10000], y[10000:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b35ac",
   "metadata": {
    "papermill": {
     "duration": 0.003957,
     "end_time": "2025-07-11T07:55:08.696643",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.692686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2  Utility functions\n",
    "Activation, loss, and padding. Documentation is generated by ChatGPT 4o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20a5b73e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.706110Z",
     "iopub.status.busy": "2025-07-11T07:55:08.705772Z",
     "iopub.status.idle": "2025-07-11T07:55:08.712959Z",
     "shell.execute_reply": "2025-07-11T07:55:08.712100Z"
    },
    "papermill": {
     "duration": 0.013637,
     "end_time": "2025-07-11T07:55:08.714425",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.700788",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Element-wise Sigmoid activation function.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    \"\"\"\n",
    "    Derivative of the sigmoid function.\n",
    "    Used for backpropagation.\n",
    "    \"\"\"\n",
    "    s = sigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(Z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute softmax probabilities over classes.\n",
    "\n",
    "    Parameters:\n",
    "    - Z: logits of shape (batch_size, num_classes)\n",
    "\n",
    "    Returns:\n",
    "    - softmax probabilities (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    Z = Z - np.max(Z, axis=1, keepdims=True)\n",
    "    exp_Z = np.exp(Z)\n",
    "    return exp_Z / np.sum(exp_Z, axis=1, keepdims=True)\n",
    "\n",
    "def CrossEntropy(yhat: np.ndarray, y: np.ndarray, eps: float = 1e-15) -> float:\n",
    "    \"\"\"\n",
    "    Mean cross-entropy loss for multi-class classification.\n",
    "\n",
    "    Parameters:\n",
    "    - yhat: Predicted probabilities (batch_size, num_classes)\n",
    "    - y: One-hot encoded true labels (batch_size, num_classes)\n",
    "    - eps: Small epsilon to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "    - Mean cross-entropy loss over the batch\n",
    "    \"\"\"\n",
    "    yhat = np.clip(yhat, eps, 1 - eps)\n",
    "    return -np.mean(np.sum(y * np.log(yhat), axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4379f0ad",
   "metadata": {
    "papermill": {
     "duration": 0.003726,
     "end_time": "2025-07-11T07:55:08.722243",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.718517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Zero Padding\n",
    "\n",
    "Pad the input to get the 32×32 images LeNet-5 was designed for.  \n",
    "This function does zero-padding around the spatial dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60b82b81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.732450Z",
     "iopub.status.busy": "2025-07-11T07:55:08.732074Z",
     "iopub.status.idle": "2025-07-11T07:55:08.737274Z",
     "shell.execute_reply": "2025-07-11T07:55:08.736400Z"
    },
    "papermill": {
     "duration": 0.011601,
     "end_time": "2025-07-11T07:55:08.738711",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.727110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding(X: np.ndarray, p: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Pads images with zeros.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input tensor of shape (batch, channels, height, width)\n",
    "    - p: Padding size\n",
    "\n",
    "    Returns:\n",
    "    - Zero-padded tensor of shape (batch, channels, height + 2*p, width + 2*p)\n",
    "    \"\"\"\n",
    "    batch, channel, height, width = X.shape\n",
    "    Y = np.zeros((batch, channel, height + 2*p, width + 2*p))\n",
    "    Y[:, :, p:p+height, p:p+width] = X\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513cf81",
   "metadata": {
    "papermill": {
     "duration": 0.003782,
     "end_time": "2025-07-11T07:55:08.746723",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.742941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolution Layer (Forward)\n",
    "\n",
    "This function performs a multi-channel, multi-filter convolution, padding inputs as needed.\n",
    "We implement the forward pass using for loops, summing over input channels and applying each filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e56938fe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.756373Z",
     "iopub.status.busy": "2025-07-11T07:55:08.756041Z",
     "iopub.status.idle": "2025-07-11T07:55:08.763216Z",
     "shell.execute_reply": "2025-07-11T07:55:08.762347Z"
    },
    "papermill": {
     "duration": 0.013952,
     "end_time": "2025-07-11T07:55:08.764755",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.750803",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Convolution(X, p, in_channels, out_channels, K, bias):\n",
    "    \"\"\"\n",
    "    Performs a convolution over multi-channel input with multiple output channels.\n",
    "    \n",
    "    Arguments:\n",
    "    - X: input tensor of shape (batch, in_channels, height, width)\n",
    "    - padding: number of padding pixels on each side\n",
    "    - in_channels: number of input channels\n",
    "    - out_channels: number of output channels (number of filters)\n",
    "    - K: Kernel\n",
    "    - bias: Bias term added to each output channel\n",
    "    \n",
    "    Returns:\n",
    "    - Y: output tensor of shape (batch, out_channels, output_height, output_width)\n",
    "    - K: The kernel to be reused\n",
    "    - b: The bias to be reused\n",
    "    \"\"\"\n",
    "\n",
    "    # Pad input\n",
    "    if p > 0:\n",
    "        X = padding(X, p)\n",
    "\n",
    "    batch, channel, height, width = X.shape\n",
    "\n",
    "    # Compute output size (assuming stride=1)\n",
    "    output_height = height - K.shape[2] + 1\n",
    "    output_width  = width - K.shape[3] + 1\n",
    "    Y = np.zeros(shape=(batch, out_channels, output_height, output_width))\n",
    "\n",
    "    \"\"\"\n",
    "    First attempt. Worked but was really slow. Kept here to showcase my struggles.\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            for in_ch in range(in_channels):\n",
    "                for h in range(output_height):\n",
    "                    for w in range(output_width):\n",
    "                        Y[b, out_ch, h, w] += np.sum(\n",
    "                            X[b, in_ch, h: h + K.shape[2], w: w + K.shape[3]] * \n",
    "                            K[out_ch, in_ch]\n",
    "            Y[b, out_ch, :, :] += bias[out_ch]\n",
    "    \"\"\"\n",
    "    # Vectorized height and width loop.-\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            accum = np.zeros((output_height, output_width))\n",
    "            # Sum over each input channel\n",
    "            for in_ch in range(in_channels):\n",
    "                # This uses a 2D sliding window, no inner h,w loop\n",
    "                for i in range(K.shape[2]):\n",
    "                    for j in range(K.shape[3]):\n",
    "                        accum += X[b, in_ch, i:i+output_height, j:j+output_width] * K[out_ch, in_ch, i, j]\n",
    "            # Add bias\n",
    "            Y[b, out_ch] = accum + bias[out_ch]\n",
    "\n",
    "    return Y, K, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee02211b",
   "metadata": {
    "papermill": {
     "duration": 0.003796,
     "end_time": "2025-07-11T07:55:08.772796",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.769000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Convolution Backward Pass\n",
    "\n",
    "Computes gradients for weights, biases, and inputs.\n",
    "This is a direct implementation of the chain rule for convolution layers. This was by far the most difficult part of the project and many errors appeared.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2f1e2cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.782147Z",
     "iopub.status.busy": "2025-07-11T07:55:08.781791Z",
     "iopub.status.idle": "2025-07-11T07:55:08.790762Z",
     "shell.execute_reply": "2025-07-11T07:55:08.789804Z"
    },
    "papermill": {
     "duration": 0.015558,
     "end_time": "2025-07-11T07:55:08.792447",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.776889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ConvBackward(dY, X, K):\n",
    "    \"\"\"\n",
    "    Backpropagation for a convolution layer.\n",
    "\n",
    "    Computes gradients w.r.t.:\n",
    "    - input X\n",
    "    - kernel weights K\n",
    "    - bias\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient of loss w.r.t. output, shape (batch, out_channels, out_height, out_width)\n",
    "    - X: Input to the convolution, shape (batch, in_channels, height, width)\n",
    "    - K: Kernels, shape (out_channels, in_channels, kH, kW)\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient w.r.t. input X\n",
    "    - dK: Gradient w.r.t. kernels\n",
    "    - db: Gradient w.r.t. biases (averaged over batch)\n",
    "    \"\"\"\n",
    "    batch, in_channels, height, width = X.shape\n",
    "    out_channels, _, kH, kW = K.shape\n",
    "    out_height, out_width = dY.shape[2], dY.shape[3]\n",
    "\n",
    "    dK = np.zeros_like(K)\n",
    "    db = np.sum(dY, axis=(0,2,3)) / batch\n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for b in range(batch):\n",
    "        for out_ch in range(out_channels):\n",
    "            for in_ch in range(in_channels):\n",
    "                for i in range(kH):\n",
    "                    for j in range(kW):\n",
    "                        h_end = i + out_height\n",
    "                        w_end = j + out_width\n",
    "                        # clip if we overshoot the input dims\n",
    "                        if h_end > height or w_end > width:\n",
    "                            h_end = min(h_end, height)\n",
    "                            w_end = min(w_end, width)\n",
    "                            slice_h = h_end - i\n",
    "                            slice_w = w_end - j\n",
    "                            # also slice dY to match\n",
    "                            dY_slice = dY[b, out_ch, :slice_h, :slice_w]\n",
    "                            X_slice = X[b, in_ch, i:h_end, j:w_end]\n",
    "                        else:\n",
    "                            X_slice = X[b, in_ch, i:h_end, j:w_end]\n",
    "                            dY_slice = dY[b, out_ch]\n",
    "\n",
    "                        # Accumulate gradients for kernel weights\n",
    "                        dK[out_ch, in_ch, i, j] += np.sum(X_slice * dY_slice) / batch\n",
    "\n",
    "                         # Accumulate gradients for input\n",
    "                        dX[b, in_ch, i:h_end, j:w_end] += K[out_ch, in_ch, i, j] * dY_slice\n",
    "    return dX, dK, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9be62f",
   "metadata": {
    "papermill": {
     "duration": 0.003987,
     "end_time": "2025-07-11T07:55:08.800894",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.796907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Average Pooling Layer (Forward)\n",
    "\n",
    "Implements average pooling with a specified kernel size and stride.\n",
    "This uses a vectorized sliding window summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "234b2f12",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.810645Z",
     "iopub.status.busy": "2025-07-11T07:55:08.810340Z",
     "iopub.status.idle": "2025-07-11T07:55:08.817142Z",
     "shell.execute_reply": "2025-07-11T07:55:08.816218Z"
    },
    "papermill": {
     "duration": 0.013665,
     "end_time": "2025-07-11T07:55:08.818776",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.805111",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Pooling(X, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Average pooling operation.\n",
    "\n",
    "    Parameters:\n",
    "    - X: Input tensor of shape (batch, channel, height, width)\n",
    "    - kernel_size: Tuple (kH, kW) specifying pooling window size\n",
    "    - stride: Stride of the pooling operation\n",
    "\n",
    "    Returns:\n",
    "    - Y: Output tensor after pooling\n",
    "    \"\"\"\n",
    "\n",
    "    batch, channel, height, width = X.shape\n",
    "    output_height = (height - kernel_size[0]) // stride + 1\n",
    "    output_width  = (width - kernel_size[1]) // stride + 1\n",
    "    Y = np.zeros(shape=(batch, channel, output_height, output_width))\n",
    "\n",
    "    \"\"\"\n",
    "    First attempt, too slow.\n",
    "    for b in range(batch):\n",
    "        for ch in range(channel):\n",
    "            for h in range(output_height):\n",
    "                for w in range(output_width):\n",
    "                    # Calculated this from a piece of paper, helped me understand everything a lot better.\n",
    "                    Y[b, ch, h, w] = np.mean(X[b, ch, \n",
    "                                               (h * stride): (h * stride) + kernel_size[0],\n",
    "                                               (w * stride): (w * stride) + kernel_size[1]])\n",
    "    \"\"\"\n",
    "\n",
    "    window_sum = np.zeros((batch, channel, output_height, output_width))\n",
    "    for i in range(kernel_size[0]):\n",
    "        for j in range(kernel_size[1]):\n",
    "            window_sum += X[:, :, i:i+output_height*stride:stride, j:j+output_width*stride:stride]\n",
    "    Y = window_sum / (kernel_size[0] * kernel_size[1])\n",
    "\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b3e176",
   "metadata": {
    "papermill": {
     "duration": 0.003918,
     "end_time": "2025-07-11T07:55:08.827072",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.823154",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Average Pooling Backward Pass\n",
    "\n",
    "Distributes gradients equally over the region covered by each pooling window.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "754fdab4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.836670Z",
     "iopub.status.busy": "2025-07-11T07:55:08.836366Z",
     "iopub.status.idle": "2025-07-11T07:55:08.842972Z",
     "shell.execute_reply": "2025-07-11T07:55:08.842000Z"
    },
    "papermill": {
     "duration": 0.013365,
     "end_time": "2025-07-11T07:55:08.844553",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.831188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def PoolingBackward(dY, X, kernel_size, stride):\n",
    "    \"\"\"\n",
    "    Backpropagation for average pooling.\n",
    "\n",
    "    Spreads gradient uniformly over the region that was pooled.\n",
    "\n",
    "    Parameters:\n",
    "    - dY: Gradient w.r.t. output, shape (batch, channel, out_height, out_width)\n",
    "    - X: Input to pooling layer, shape (batch, channel, height, width)\n",
    "    - kernel_size: Tuple (kH, kW)\n",
    "    - stride: Pooling stride\n",
    "\n",
    "    Returns:\n",
    "    - dX: Gradient w.r.t. input X\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, channel, height, width = X.shape\n",
    "    output_height, output_width = dY.shape[2], dY.shape[3]\n",
    "    kH, kW = kernel_size\n",
    "    dX = np.zeros_like(X)\n",
    "\n",
    "    for b in range(batch):\n",
    "        for ch in range(channel):\n",
    "            for h in range(output_height):\n",
    "                for w in range(output_width):\n",
    "                    dX[b, ch,\n",
    "                       (h * stride):(h * stride) + kH,\n",
    "                       (w * stride):(w * stride) + kW] += dY[b, ch, h, w] / (kH * kW)\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a63619",
   "metadata": {
    "papermill": {
     "duration": 0.004047,
     "end_time": "2025-07-11T07:55:08.853096",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.849049",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Xavier Initialization\n",
    "\n",
    "We initialize all kernels (weights) with Xavier initialization, which sets weights in a range depending on the number of input and output connections. This helps maintain stable variance across layers.\n",
    "\n",
    "We also initialize biases to zeros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1405f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.862473Z",
     "iopub.status.busy": "2025-07-11T07:55:08.862152Z",
     "iopub.status.idle": "2025-07-11T07:55:08.866992Z",
     "shell.execute_reply": "2025-07-11T07:55:08.866091Z"
    },
    "papermill": {
     "duration": 0.01142,
     "end_time": "2025-07-11T07:55:08.868506",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.857086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def xavier_init(size, n_in, n_out):\n",
    "    \"\"\"\n",
    "    Xavier (Glorot) initialization for weights.\n",
    "\n",
    "    Parameters:\n",
    "    - size: Shape of the weight tensor\n",
    "    - n_in: Number of input units\n",
    "    - n_out: Number of output units\n",
    "\n",
    "    Returns:\n",
    "    - np.ndarray: Initialized weights\n",
    "    \"\"\"\n",
    "    \n",
    "    limit = np.sqrt(6 / (n_in + n_out))\n",
    "    return np.random.uniform(-limit, limit, size=size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b5c2487",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.878541Z",
     "iopub.status.busy": "2025-07-11T07:55:08.877851Z",
     "iopub.status.idle": "2025-07-11T07:55:08.884864Z",
     "shell.execute_reply": "2025-07-11T07:55:08.884105Z"
    },
    "papermill": {
     "duration": 0.013873,
     "end_time": "2025-07-11T07:55:08.886583",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.872710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Xavier initialization for each layer\n",
    "\n",
    "# C1: 6 filters, 1 input channel, 5x5 kernel\n",
    "c1_kernel = xavier_init(size=(6, 1, 5, 5), n_in=25, n_out=150)\n",
    "c1_bias = np.zeros(6)\n",
    "\n",
    "# C3: 16 filters, 6 input channels\n",
    "c3_kernel = xavier_init(size=(16, 6, 5, 5), n_in=150, n_out=400)\n",
    "c3_bias = np.zeros(16)\n",
    "\n",
    "# C5: 120 filters, 16 input channels\n",
    "c5_kernel = xavier_init(size=(120, 16, 5, 5), n_in=400, n_out=3000)\n",
    "c5_bias = np.zeros(120)\n",
    "\n",
    "# F6: fully connected 120 -> 84\n",
    "f6_weights = xavier_init((84, 120), n_in=120, n_out=84)\n",
    "f6_bias = np.zeros(84)\n",
    "\n",
    "# Output layer: fully connected 84 -> 10\n",
    "out_weights = xavier_init((10, 84), n_in=84, n_out=10)\n",
    "out_bias = np.zeros(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8685619",
   "metadata": {
    "papermill": {
     "duration": 0.003956,
     "end_time": "2025-07-11T07:55:08.894890",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.890934",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Loop\n",
    "\n",
    "Trains the network over multiple epochs. Uses SGD to update weights.\n",
    "Includes explicit forward and backward passes through all layers.\n",
    "\n",
    "We also shuffle the data each epoch to improve convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f552acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-11T07:55:08.904474Z",
     "iopub.status.busy": "2025-07-11T07:55:08.904177Z",
     "iopub.status.idle": "2025-07-11T12:13:41.512572Z",
     "shell.execute_reply": "2025-07-11T12:13:41.510969Z"
    },
    "papermill": {
     "duration": 15512.618726,
     "end_time": "2025-07-11T12:13:41.517709",
     "exception": false,
     "start_time": "2025-07-11T07:55:08.898983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.0363 | Accuracy: 0.1041\n",
      "Epoch 2 | Loss: 0.0363 | Accuracy: 0.1032\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "epochs = 2\n",
    "lr = 0.1\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0    # Sum of losses for this epoch\n",
    "    correct = 0       # Count of correct predictions\n",
    "\n",
    "    # Shuffle data each epoch\n",
    "    perm = np.random.permutation(X_train.shape[0])\n",
    "    X_train = X_train[perm]\n",
    "    y_train = y_train[perm]\n",
    "    \n",
    "    for batch in range(0, X_train.shape[0], batch_size):\n",
    "        end = min(batch + batch_size, X_train.shape[0])\n",
    "        X_train_batch = X_train[batch:end]\n",
    "        y_train_batch = y_train[batch:end]\n",
    "        batch_len = X_train_batch.shape[0]\n",
    "    \n",
    "        if batch_len == 0:\n",
    "            continue\n",
    "        # Forward Pass\n",
    "        c1, c1_kernel, c1_bias = Convolution(X_train_batch,\n",
    "                                             p=2, \n",
    "                                             in_channels=1, \n",
    "                                             out_channels=6,\n",
    "                                             K=c1_kernel,\n",
    "                                             bias=c1_bias)\n",
    "        a1 = sigmoid(c1)\n",
    "        s2 = Pooling(a1, kernel_size=(2, 2), stride=2)\n",
    "        c3, c3_kernel, c3_bias = Convolution(s2,\n",
    "                                             p=0,\n",
    "                                             in_channels=6,\n",
    "                                             out_channels=16,\n",
    "                                             K=c3_kernel,\n",
    "                                             bias=c3_bias\n",
    "                                             )\n",
    "        a3 = sigmoid(c3)\n",
    "        s4 = Pooling(a3, kernel_size=(2, 2), stride=2)\n",
    "        c5, c5_kernel, c5_bias = Convolution(s4,\n",
    "                                             p=0,\n",
    "                                             in_channels=16,\n",
    "                                             out_channels=120,\n",
    "                                             K=c5_kernel,\n",
    "                                             bias=c5_bias\n",
    "                                             )\n",
    "        a5 = sigmoid(c5).reshape(batch_len, 120)\n",
    "        f6 = np.dot(a5, f6_weights.T) + f6_bias\n",
    "        a6 = sigmoid(f6)\n",
    "        output = np.dot(a6, out_weights.T) + out_bias\n",
    "        yhat = softmax(output)\n",
    "\n",
    "        # Onehot Encoding Labels\n",
    "        y_onehot = np.zeros((batch_len, 10))\n",
    "        y_onehot[np.arange(batch_len), y_train_batch] = 1   # Turn class indices into one-hot vectors\n",
    "\n",
    "        # Loss and Accuracy\n",
    "        loss = CrossEntropy(yhat, y_onehot)                 # Average loss over batch\n",
    "        total_loss += loss\n",
    "        preds = np.argmax(yhat, axis=1)                     # Class prediction per sample\n",
    "        correct += np.sum(preds == y_train_batch)           # Tally correct predictions\n",
    "\n",
    "        dz_output = yhat - y_onehot\n",
    "        # Grad for output layer\n",
    "        dw_out = np.dot(dz_output.T, a6) / batch_len  # shape (10, 84)\n",
    "        db_out = dz_output.sum(axis=0) / batch_len    # shape (10,)\n",
    "\n",
    "        # Propagate to hidden layer\n",
    "        da6 = np.dot(dz_output, out_weights)          # (batch, 84)\n",
    "        dz6 = da6 * sigmoid_deriv(f6)                 # (batch, 84)\n",
    "\n",
    "        # Grad for F6 layer\n",
    "        dw_f6 = np.dot(dz6.T, a5) / batch_len         # shape (84, 120)\n",
    "        db_f6 = dz6.sum(axis=0) / batch_len           # shape (84,)\n",
    "\n",
    "        # Propagate to C5 output\n",
    "        da5 = np.dot(dz6, f6_weights)                 # (batch, 120)\n",
    "        da5 = da5.reshape(batch_len, 120, 1, 1)       # for compatibility with conv\n",
    "\n",
    "        # Propagate through C5 layer\n",
    "        dz5 = da5 * sigmoid_deriv(c5)\n",
    "        ds4, dK5, db5 = ConvBackward(dz5, s4, c5_kernel)\n",
    "\n",
    "        # Propagate through S4\n",
    "        da3 = PoolingBackward(ds4, a3, (2,2), 2)\n",
    "\n",
    "        #Propagate through C3\n",
    "        ds2, dK3, db3 = ConvBackward(da3 * sigmoid_deriv(c3), s2, c3_kernel)\n",
    "\n",
    "        #Propagate through S2\n",
    "        da1 = PoolingBackward(ds2, a1, (2,2), 2)\n",
    "\n",
    "        #Propagate through C1\n",
    "        _, dK1, db1 = ConvBackward(da1 * sigmoid_deriv(c1), X_train_batch, c1_kernel)\n",
    "\n",
    "        # Adjusting Parameters\n",
    "        out_weights -= lr * dw_out\n",
    "        out_bias -= lr * db_out\n",
    "        f6_weights -= lr * dw_f6\n",
    "        f6_bias -= lr * db_f6\n",
    "        c5_kernel -= lr * dK5\n",
    "        c5_bias -= lr * db5\n",
    "        c3_kernel -= lr * dK3\n",
    "        c3_bias -= lr * db3\n",
    "        c1_kernel -= lr * dK1\n",
    "        c1_bias -= lr * db1\n",
    "\n",
    "    # Epoch Results\n",
    "    acc = correct / X_train.shape[0]\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / X_train.shape[0]:.4f} | Accuracy: {acc:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c89f0b",
   "metadata": {
    "papermill": {
     "duration": 0.004079,
     "end_time": "2025-07-11T12:13:41.526263",
     "exception": false,
     "start_time": "2025-07-11T12:13:41.522184",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "In this project, I successfully implemented the classic LeNet-5 architecture both from scratch in NumPy (including manual forward and backward passes for convolutions, pooling, and fully connected layers) and then replicated it in PyTorch, using GPU acceleration for practical training.\n",
    "\n",
    "I was disappointed that I couldn’t complete the training purely from scratch using my NumPy implementation on a laptop CPU. I added vectorization and after over 12 hours of runtime, not even a single epoch had completed. This is a clear demonstration of how slow manual loops are without optimizations like im2col.\n",
    "\n",
    "This experience reinforced just how critical optimized libraries (like PyTorch or TensorFlow) are for deep learning. They hide these costly operations behind efficient CUDA kernels and parallelized CPU operations, enabling models like LeNet-5 to train in minutes rather than days.\n",
    "\n",
    "Overall, despite the challenge of not being able to fully train my from-scratch model, this project gave me deep insights into the mathematical operations, data flows, and practical engineering trade-offs behind convolutional neural networks."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15548.761925,
   "end_time": "2025-07-11T12:13:42.166693",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-11T07:54:33.404768",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
