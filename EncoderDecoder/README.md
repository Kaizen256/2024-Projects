# Sequence Prediction with Self-Attention Model


This project demonstrates sequence prediction using a custom Encoder-Decoder model with self-attention mechanisms. The model is trained on sequences generated by the generate_sequences function and predicts future points in the sequence. This project uses PyTorch. This was also followed through a book.


Encoder: Encodes the input sequence using self-attention and positional encoding.
Decoder: Decodes the encoded sequence to predict the future points.
Self-Attention Mechanism: Utilizes multi-head attention for capturing dependencies within sequences.
Positional Encoding: Adds positional information to the sequence embeddings.


1. Generates training and test sequences using a custom sequence generator.
2. Creates data loaders for training and testing datasets.
3. Defines a custom Encoder-Decoder model with self-attention layers.
4. Implements positional encoding to enhance sequence representations.
5. Trains the model and evaluates its performance.
6. Prints the training and testing loss after training.


Loss Function: Mean Squared Error Loss.
Optimizer: Adam optimizer.
Training: Configurable number of epochs, with periodic loss printing
